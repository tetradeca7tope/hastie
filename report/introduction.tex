

\section{Introduction}

Regression in high dimensions is an inherently difficult problem with known
lower bounds depending exponentially in dimension
\citep{gyorfi02distributionfree}. In this
project we intend to make progress in this problem by treating the function as
an additive model of lower dimensional components.
Using additive models is fairly standard in high dimensional regression
literature \cite{hastie90gam,ravikumar09spam,lafferty05rodeo}. 
However, we wish to consider additive models which are more expressive than
previous work.

We choose to model the low-order interaction basis functions implicitly using kernels.
We then minimize the squared-error loss combined with 
a squared RKHS norm penalty to enforce smoothness
and a mixed $\ell_{1,2}$-norm (group lasso style) penalty 
to limit the number of non-zero basis functions.
This leads to a convex objective function where the number of parameters is
the product of the number of samples and the number of basis functions.

Our work extends Sparse Additive Models (SpAM) \citep{ravikumar09spam} 
to multidimensional nonparametric basis functions.
Our proposed method also extends recent work on 
Generalized Additive Models plus Interactions \citep{intelligible:2013}.
However, in this work the interaction model was assumed to follow a specific functional form,
leading to an optimization method tailored to their interaction model.
Our research is also related to existing work on 
using linear combinations of kernels for kernel learning,
called multiple kernel learning \citep{mkl-review:2011}.

Optimization for our proposed method is complicated by 
the non-smooth $\ell_{1,2}$-norm regularization penalty.
Algorithms for group lasso have addressed this problem 
through a variety of approaches.
Proximal gradient \citep{beck2009fast}
has cheap iterations and relatively fast convergence if combined with acceleration.
An block coordinate descent method has also been developed \citep{bcd-group-lasso:2013}.
Further, the general Coordinate Gradient Descent method \citep{cgd:2009} 
can also be specialized to $\ell_{1,2}$-penalized problems 
\citep{meier2008group,note-group-lasso:2010}.
Recent work \citep{group-fused-lasso:2014} on the group fused lasso 
has sidestepped the $\ell_{1,2}$-norm penalty, transforming it to a 
smooth objective with non-negativity constraint.
Finally, safe screening rules for the group lasso have been developed 
\citep{group-lasso-screening:2013} which quickly eliminate many of the all-0 
groups.

For Sparse Additive Models, parameters are typically 
optimized via the backfitting algorithm \citep{ravikumar09spam}, 
a special case of (block) coordinate descent with group sizes of 1.

