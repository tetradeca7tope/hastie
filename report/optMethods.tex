
\section{Optimisation Methods}
\label{sec:optMethods}

\subsubsection*{Subgradient Method}
Because our objective function is non-smooth, the simplest method is to solve it via 
the subgradient method. We implemented subgradient method with 
decreasing step sizes for our experiments. 
While subgradients can be computed cheaply in $O(nM)$, we observed poor convergence on 
our problem, as we would expect.

\subsubsection*{Proximal Gradient Method}

Note that we can write the objective \eqref{eqn:optObjective} as $F(\aalpha) =
G(\aalpha) + \Psi(\aalpha)$ where $G$ is smooth and $\Psi$ is not. $\Psi$ is the
popular group lasso penalty. Via the Moreau decomposition, and using the fact
that the argument in the prox operator is separable, the prox operator can
be shown to be,
\[
[\prox_{\Psi, t} (\aalpha)]^{(j)} =
\prox_{\Psi, t} (\alphaj) = 
  \begin{cases}
    \alphaj - t \frac{\alpha}{\|\alphaj\|_2} & \text{if } 
              \|\alphaj\| < t \\
    \zero & \text{otherwise}
  \end{cases}
\]
Obtaining the prox operator takes $O(nM)$ time and has cost comparable to
computing the gradient of $G(\aalpha)$. We use the above to implement proximal
gradient method as described in the class notes. We use backtracking to
determine the step size and experimented both with and without acceleration.

\subsubsection*{Exact Block Coordinate Descent via Newton trust region}

We can optimize the objective function over group $\alphaj$ with all other groups
$\alphai, i \ne j$ fixed. The objective is then
\[
\arg\min_\alphaj \frac{1}{2}\alphaj^T A_j \alphaj + b_j^T \alphaj + \lambda_2 \|\alphaj\|_2
\]
where $A_j = \KKj^T \KKj + \frac{\lambda_1}{2}\KKj$ 
and $b_j = -\KKj^T (y-\sum_{i\ne j}{\KKi \alphai}$.
This problem can be efficiently solved \citep{bcd-group-lasso:2013} 
by converting it to a one-dimensional trust-region problem. 
When $\|b_j\|_2 \le \lambda_2$, we have $\alphaj = 0$. 
Otherwise, there exists some $t_j$ such that $\alphaj$ is the solution of the following
problem
\[
\arg\min_\alphaj \frac{1}{2}\alphaj^T A_j \alphaj + b_j^T \alphaj, 
\text{ such that } \|\alphaj\|_2 \le t_j.
\]
If $t_j$ is known we have $\alphaj = t_j * \left(-(t_j A_j + \lambda_2 I_n)^{-1} b_j\right)$
Because in this case $\|\alphaj\|_2 = t_j$, we have $\|(t_j A_j + \lambda_2 I_n)^{-1} b_j\|_2 = 1$.
As described in \citet{bcd-group-lasso:2013}, 
we can use Newton's method to solve this equation efficiently 
using the eigendecomposition of $A_j$, which is constant and needs to be computed only once.
However, this method does not scale well with $n$, since we still need
to solve an $n \times n$ linear system after $t_j$ is computed.
Thus, updating all blocks costs $O(n^3 M)$.

\subsubsection*{Block Coordinate Gradient Descent}

Because $\Psi(\aalpha)$, our $\ell_{1,2}$ group lasso penalty, is non-smooth yet block-separable,
we can apply the Coordinate Gradient Descent method \citep{cgd:2009}.
For each block of $\alphaj$, we solve
\[
\arg\min_{d_j} \frac{1}{2}d_j^T H_j d_j + \nabla_{j}G(\aalpha)^T d_j 
+ \lambda_2 \|\alphaj+d_j\|_2
\]
where $H_j \approx \nabla^2_{jj}G(\aalpha) = \KKj^T \KKj + \frac{\lambda_1}{2}\KKj$ 
and $\nabla_j G(\aalpha) = -\KKj^T (y-\sum_{i\ne j}{\KKi \alphai})$.
As suggested \citet{cgd:2009}, we use a diagonal matrix to approximate the true block Hessian.
We set $H_j = \max(\diag(\nabla^2_{jj}G(\aalpha))) I_n := h_j I_n$, so that a closed-form solution exists
for $d_j$:
\[
d_j = \frac{1}{h_j}\left(\lambda_2 \frac{\nabla_j G(\aalpha) - h_j \alphaj}
{\|\nabla_j G(\aalpha) - h_j \alphaj\|_2}\right).
\]
We use backtracking to determine the step size, then update $\alphaj \leftarrow \alphaj + t d_j$.

Because the Hessian is constant, each block update costs $O(n)$ and updating all blocks costs $O(nM)$.
Alternatively, as suggested in \citet{note-group-lasso:2010}, rather than using the 
diagonal Hessian approximation, we could approximately solve each block subproblem via 
coordinate descent.
