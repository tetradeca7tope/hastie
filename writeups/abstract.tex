
\begin{abstract}
We describe additive kernel regression (\addkrr), 
a generalisation of kernel least squares methods for nonparametric regression.
Nonparametric methods typically allow us to consider a richer class of functions
over parametric methods. However, unlike their parametric
counterparts they suffer from high sample complexity  in high
dimensions and cannot be used to identify structure in the
function.
A common assumption in high dimensional regression models is to assume that the
function is additive. In this work, we leverage this assumption, but 
considerably generalise existing additive models.
We propose a convex optimisation objective for our problem and optimise it using
Block Coordinate Gradient Descent.
We demonstrate
that \addkrrs significantly outperforms existing algorithms for nonparametric
regression on moderate to high dimensional problems and can be used to identify
and exploit structure in the function.
\end{abstract}
