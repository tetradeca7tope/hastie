
\section{Conclusion}
\label{sec:conclusion}

We proposed a framework for additive least squares regression. We design our
estimate to be a sum of functions where the functions are obtained by jointly
optimising over several RKHSs.
The proposed framework is useful for high dimensional nonparametric regression
since it provides favourable bias variance tradeoffs in high dimensions.
Further, it can also be used for the recovery of sparse functions when the
underlying function is additive.
Our initial experimental results indicate that our methods are superior or
competitive with existing methods in both fronts.

Going forward, we wish to study the theoretical properties of such penalized
additive models especially focusing on rate of convergence and sparsistency.


