\section{Experiments}
\label{sec:experiments}

\insertTableRealData

We present empirical results on synthetic and real datasets in both settings
described above.

\subsection{Setting 1: ESP Kernels}

In our implementations, for the one dimensional base kernel we use
the RBF kernel $\kerni(x,x') = \exp((x-x')^2/h^2)$ with bandwidth $h$.
Since cross validating on all the kernel bandwidths is expensive, we set
it to $h = c\sigma n^{-0.2}$. This follows other literature 
\cite{gyorfi02distributionfree,ravikumar09spam} using similar choices for kernel
bandwidths. The constant $c$ was hand tuned -- we found that the performance of
our methods was robust to choices of $c$ between $5$ and $40$.
The value of $M$ was also hand tuned and set to $M = \min(D/4, 10)$.
In this setting we compare \addkrrs against kernel kidge regression(\krr),
Nadaraya Watson regression (\nw), locally linear regression (\locallin), locally
quadratic regression (\localquad), Gaussian process regression (\gp), $k$
nearest neighbors regression (\knn) and support vector regression (\svr).
For \gps and \svrs we use the implementations in
\citet{rasmussen10gpml,chang11libsvm} respectively.
For the other methods, we chose hyper parameters using $5$-fold cross
validation.
The Additive Gaussian process model of \citep{duvenaud11additivegps} is also a
candidate but we found that inference was extremely slow beyond a few hundred
training points.

First, we construct a smooth synthetic 20 dimensional function. We train all methods on
$n$ training points where $n$ varies from $100$ to $1100$ and test on $1000$
points sampled independently. The results are shown in Figure~\ref{fig:compToy}.
\addkrrs outperforms all other methods. We suspect that \nw, \locallin and \knn perform
very poorly since they make very weak smoothness assumptions about the function.

Next, we compare all methods on $7$ high dimensional datasets from the UCI
repository. All inputs and labels were preprocessed to have zero mean and unit
standard deviation. We split the datasets into roughly two halves for training
and testing. The results are given in Table~\ref{tb:realData}. Our proposed
method outperforms alternatives in most cases.

\insertFigCompToy

\input{functionsel}

