
\section{Implementation}
\label{sec:implementation}

We now describe the implementation and the optimisation procedures used therein.
Let the Cholesky decomposition of $\KKj$ be $\KKj = \LLj \LLj^\top$. 
Denote $\betaj = \LLj^\top \alphaj$.
Then, our objective can be written in terms of $\bbeta = [{\betaii{1}}^\top,
\dots, {\betaii{M}}^\top]$ as,
\begin{equation}
\Fbeta(\bbeta) =  \frac{1}{2}\Big\|Y - \sum_{j=1}^m \LLj \alphaj\Big\|_2^2 + 
  \lambda \sum_{j=1}^M \|\betaj\|_2
\label{eqn:objBeta}
\end{equation}
The objective, in the above form is well studied in optimisation literature as the group
LASSO. 
We consider the following methods to optimise the objectives
in~\eqref{eqn:optObjective} and~\eqref{eqn:objBeta}.

\subsubsection*{Subgradient Method on $\bbeta$~\eqref{eqn:objBeta}}
Because our objective function is non-smooth, the simplest method is to solve it via 
the subgradient method. We implemented subgradient method with 
decreasing step sizes for our experiments. 
While subgradients can be computed cheaply in $O(nM)$, we observed poor 
convergence on our problem. %, as we would expect.

\subsubsection*{Subgradient Method on $\aalpha$~\eqref{eqn:optObjective}}
Here, we directly performed subgradient method on the objective
in~\eqref{eqn:optObjective}. The advantage to this method is that it doesn't
require the potentially expensive Cholesky decompositions at the start of the
algorithm. Despite this, we saw that convergence was slow.

\subsubsection*{Proximal Gradient Method}

Note that we can write the objective \eqref{eqn:optObjective} as $F(\bbeta) =
G(\bbeta) + \Psi(\bbeta)$ where $G$ is smooth and $\Psi$ is not. $\Psi$ is the
group lasso penalty. Via the Moreau decomposition, and using the fact
that the argument in the prox operator is separable, the prox operator can
be shown to be,
\[
[\prox_{\Psi, t} (\bbeta)]^{(j)} =
\prox_{\Psi, t} (\betaj) = 
  \begin{cases}
    \alphaj - t \frac{\beta}{\|\betaj\|_2} & \text{if } 
              \|\betaj\| < t \\
    \zero & \text{otherwise}
  \end{cases}
\]
Obtaining the prox operator takes $O(nM)$ time and has cost comparable to
computing the gradient of $G(\bbeta)$. We use the above to implement proximal
gradient method as described in the class notes. We use backtracking to
determine the step size and experimented both with and without acceleration.

\subsubsection*{Exact Block Coordinate Descent via Newton trust region}

\toworkon{@Calvin: Pls update this.}
We can optimize the objective function over group $\alphaj$ with all other groups
$\alphai, i \ne j$ fixed. The objective is then
\[
\arg\min_\alphaj \frac{1}{2}\alphaj^T A_j \alphaj + b_j^T \alphaj + \lambda_2 \|\alphaj\|_2
\]
where $A_j = \KKj^T \KKj + \frac{\lambda_1}{2}\KKj$ 
and $b_j = -\KKj^T (y-\sum_{i\ne j}{\KKi \alphai}$.
This problem can be efficiently solved \citep{bcd-group-lasso:2013} 
by converting it to a one-dimensional trust-region problem. 
When $\|b_j\|_2 \le \lambda_2$, we have $\alphaj = 0$. 
Otherwise, there exists $t_j$ such that $\alphaj$ is the solution of
the following problem
\[
\arg\min_\alphaj \frac{1}{2}\alphaj^T A_j \alphaj + b_j^T \alphaj, 
\text{ such that } \|\alphaj\|_2 \le t_j.
\]
If $t_j$ is known we have $\alphaj = t_j * \left(-(t_j A_j + \lambda_2 I_n)^{-1} b_j\right)$
Because in this case $\|\alphaj\|_2 = t_j$, we have $\|(t_j A_j + \lambda_2 I_n)^{-1} b_j\|_2 = 1$.
As described in \citet{bcd-group-lasso:2013}, 
we can use Newton's method to solve this equation efficiently 
using the eigendecomposition of $A_j$, which is constant and needs to be computed only once.
However, this method does not scale well with $n$, since we still need
to solve an $n \times n$ linear system after $t_j$ is computed.
Thus, updating all blocks costs $O(n^3 M)$.

\subsubsection*{Block Coordinate Gradient Descent}

\toworkon{@Calvin: Pls update this.}
Because $\Psi(\aalpha)$, our $\ell_{1,2}$ group lasso penalty, is non-smooth yet block-separable,
we can apply the Coordinate Gradient Descent method \citep{cgd:2009}.
For each block of $\alphaj$, we solve
\[
\arg\min_{d_j} \frac{1}{2}d_j^T H_j d_j + \nabla_{j}G(\aalpha)^T d_j 
+ \lambda_2 \|\alphaj+d_j\|_2
\]
where $H_j \approx \nabla^2_{jj}G(\aalpha) = \KKj^T \KKj + \frac{\lambda_1}{2}\KKj$ 
and $\nabla_j G(\aalpha) = -\KKj^T (y-\sum_{i\ne j}{\KKi \alphai})$.
As suggested \citet{cgd:2009}, we use a diagonal matrix to approximate the true block Hessian.
We set $H_j = \max(\diag(\nabla^2_{jj}G(\aalpha))) I_n := h_j I_n$, so that a closed-form solution exists
for $d_j$:
\[
d_j = \frac{1}{h_j}\left(\lambda_2 \frac{\nabla_j G(\aalpha) - h_j \alphaj}
{\|\nabla_j G(\aalpha) - h_j \alphaj\|_2}\right).
\]
We use backtracking to determine the step size, then update $\alphaj \leftarrow \alphaj + t d_j$.

Because the Hessian is constant, each block update costs $O(n)$ and updating all blocks costs $O(nM)$.
Alternatively, as suggested in \citet{note-group-lasso:2010}, rather than using the 
diagonal Hessian approximation, we could approximately solve each block subproblem via 
coordinate descent.


When the number of parameters for each group are small in group LASSO problems, 
block coordinate descent (BCD) is believed
to be the state of the art solver. However, in our case the number of parameters
is large -- growing linearly with $n$. In this regime BCD is slow since it
requires a matrix inversion at each step. In particular, we found that Block
Coordinate Gradient Descent (BCGD) and ADMM significantly outperformed BCD in our
experiments. 
In fact, we tried several other methods including subgradient method, proximal
gradient method and ADMM and found that BCGD/ ADMM performed best.
The results are shown in Figures~\ref{fig:optCompIter} and~\ref{fig:optCompTime}.

\insertFigOpt

The penalty coefficient $\lambda$ was chosen using $5$-fold cross validation. 
Our implementation
first solves for the largest $\lambda$. For successive $\lambda$
values, we initialise BCGD at the solution of the previous $\lambda$. This
warm starts procedure significantly speeds up the running time of the entire
training procedure.

