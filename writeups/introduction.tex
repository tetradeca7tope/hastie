

\section{Introduction}

Nonparametric regression in high dimensions is an inherently difficult problem with known
lower bounds depending exponentially in dimension
\citep{gyorfi02distributionfree}. 
With rare exceptions,
nonparametric methods typically work well only under at most $4-6$ dimensions.
In this project we intend to make progress in this problem by treating the
\emph{estimate} of the function as
an additive function of lower dimensional components.

Using additive models is fairly standard in high dimensional regression
literature
\cite{hastie90gam,ravikumar09spam,lafferty05rodeo}. 
% In this work we 
% However, we wish to consider additive models which are more expressive than
% previous work.
When the true underlying function $\func$ exhibits additive structure, using an additive
model for estimation is understandably reasonable. However, even when $\func$ is
not additive, using an additive model has its advantages. 
It is a well understoon notion in Statistics that when we only have a few samples, using a
simpler model to fit our data may give us a better tradeoff for estimation error  
against approximation error. 
This is because additive functions are \emph{statistically simpler}
than more general (non-additive) functions.
Typically, in most nonparametric regression methods using kernels such as the
Nadaraya-Watson
(\nw) estimator and Kernel Ridge Regression (\krr), the bias-variance tradeoff
is managed via the bandwidth of the kernel. 
% However, this is only one way to
% manage the tradeoff. In this work we demonstrate that 
Using an additive model
provides another ``knob" to control this tradeoff and provides significant gains
in high dimensional regression. In fact, \citet{duvenaud11additivegps}
demonstrate that using additive models in the Gaussian Process (\gp) framework
improves prediction performance.

Our methods are based on \krr, where
we choose to model the low-order interaction implicitly using kernels acting on
subsets of the coordinates (groups).
We minimize the squared-error loss with 
a squared RKHS norm penalty to enforce smoothness
and a mixed $\ell_{1,2}$-norm (group lasso style) penalty 
to enforce a sparse collection of functions.
This leads to a convex objective function where the number of parameters is
the product of the number of samples and the number of basis functions.

Our work extends Sparse Additive Models (SpAM) \citep{ravikumar09spam} 
to multidimensional nonparametric basis functions.
Our proposed method also extends recent work on 
Generalized Additive Models plus Interactions \citep{intelligible:2013}.
However, in this work the interaction model was assumed to follow a specific functional form,
leading to an optimization method tailored to their interaction model.
Our research is also related to existing work on 
using linear combinations of kernels for kernel learning,
called multiple kernel learning \citep{mkl-review:2011}.

Optimization for our proposed method is complicated by 
the non-smooth $\ell_{1,2}$-norm regularization penalty.
Algorithms for group lasso have addressed this problem 
through a variety of approaches.
Proximal gradient \citep{beck2009fast}
has cheap iterations and relatively fast convergence if combined with acceleration.
An block coordinate descent method has also been developed \citep{bcd-group-lasso:2013}.
Further, the general Coordinate Gradient Descent method \citep{cgd:2009} 
can also be specialized to $\ell_{1,2}$-penalized problems 
\citep{meier2008group,note-group-lasso:2010}.
Recent work \citep{group-fused-lasso:2014} on the group fused lasso 
has sidestepped the $\ell_{1,2}$-norm penalty, transforming it to a 
smooth objective with non-negativity constraint.
Finally, safe screening rules for the group lasso have been developed 
\citep{group-lasso-screening:2013} which quickly eliminate many of the all-0 
groups.
For Sparse Additive Models, parameters are typically 
optimized via the backfitting algorithm \citep{ravikumar09spam}, 
a special case of (block) coordinate descent with group sizes of 1.
In our work, we experiment with several optimisation methods for non-smooth
objectives. In our experiments, Block Coordinate Gradient Descent provided the
best performance.

