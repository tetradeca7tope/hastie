

\section{Introduction}

Given data $(X_i,Y_i)_{i=1}^n$ where $X_i\in \RR^D$, $Y\in\RR$ and 
$(X_i,Y_i)\sim P$, the goal of
regression methods is to estimate the regression function $f(x) = \EE_P[Y|X=x]$.
A popular method for regression is linear regression which models $f$ as a
linear combination of the variables $x$, i.e. $f(x) = w^\top x$ for some $w \in
\RR^D$. Such methods are computationally simple and have desirable
statistical properties when the problem meets the assumption. However, they are generally
too restrictive for many real problems.
Nonparametric regression refers to a suite of regression methods that only
assume smoothness of $f$.

While nonparametric methods consider a richer class of functions, they suffer
from severe drawbacks.
Nonparametric regression in high dimensions is an inherently difficult problem with known
lower bounds depending exponentially in dimension
\citep{gyorfi02distributionfree}. 
With rare exceptions,
nonparametric methods typically work well only under at most $4-6$ dimensions.
In addition they cannot be used to identify
structure in the problem. For instance, in the parametric setting algorithms
such as the LASSO and group LASSO can be used to identify a sparse subset of
variables/groups to describe the function. To the best of our knowledge, no
analogous methods exist in the nonparametric world.
In this project we intend to make progress in this problem by treating the
\emph{estimate} of the function as
an additive function-- $\funchat(\cdot) = \funcii{1}(\cdot) + \funcii{2}(\cdot) +
\dots + \funcii{M}(\cdot)$.

Using additive models is fairly standard in high dimensional regression
literature
\cite{hastie90gam,ravikumar09spam,lafferty05rodeo}. 
% In this work we 
% However, we wish to consider additive models which are more expressive than
% previous work.
When the true underlying function $\func$ exhibits additive structure, using an additive
model for estimation is understandably reasonable. However, even when $\func$ is
not additive, using an additive model has its advantages. 
It is a well understoon notion in Statistics that when we only have a few samples, using a
simpler model to fit our data may give us a better tradeoff for estimation error  
against approximation error. 
This is because additive functions are \emph{statistically simpler}
than more general (non-additive) functions.
Typically, in most nonparametric regression methods using kernels such as the
Nadaraya-Watson
(\nw) estimator and Kernel Ridge Regression (\krr), the bias-variance tradeoff
is managed via the bandwidth of the kernel. 
% However, this is only one way to
% manage the tradeoff. In this work we demonstrate that 
Using an additive model
provides another ``knob" to control this tradeoff and provides significant gains
in high dimensional regression. In fact, \citet{duvenaud11additivegps}
show that using additive models in the Gaussian Processes (\gp)
improves prediction performance.

Our methods are based on \krr.
We minimize the squared-error loss with 
an RKHS norm penalty to enforce smoothness and identify structure.
This leads to a convex objective function where the number of parameters is
the product of the number of samples and the number of basis functions.
Our work extends Sparse Additive Models (SpAM) \citep{ravikumar09spam} 
to multidimensional nonparametric basis functions.
Our proposed method also extends recent work on 
Generalized Additive Models plus Interactions \citep{intelligible:2013}.
However, in this work the interaction model was assumed to follow a specific functional form,
leading to an optimization method tailored to their interaction model.
Our research is also related to existing work on 
using linear combinations of kernels for kernel learning,
called multiple kernel learning \citep{mkl-review:2011}.

Optimization for our proposed method is complicated by 
the non-smooth $\ell_{1,2}$-norm regularization penalty.
Algorithms for group lasso have addressed this problem 
through a variety of approaches.
Proximal gradient \citep{beck2009fast}
has cheap iterations and relatively fast convergence if combined with acceleration.
An block coordinate descent method has also been developed \citep{bcd-group-lasso:2013}.
Further, the general Coordinate Gradient Descent method \citep{cgd:2009} 
can also be specialized to $\ell_{1,2}$-penalized problems 
\citep{meier2008group,note-group-lasso:2010}.
Recent work \citep{group-fused-lasso:2014} on the group fused lasso 
has sidestepped the $\ell_{1,2}$-norm penalty, transforming it to a 
smooth objective with non-negativity constraint.
Finally, safe screening rules for the group lasso have been developed 
\citep{group-lasso-screening:2013} which quickly eliminate many of the all-0 
groups.
For Sparse Additive Models, parameters are typically 
optimized via the backfitting algorithm \citep{ravikumar09spam}, 
a special case of (block) coordinate descent with group sizes of 1.
In our work, we experiment with several optimisation methods for non-smooth
objectives. In our experiments, Block Coordinate Gradient Descent provided the
best performance.

