
\section{Problem Statement}

Let $\func : \Xcal \rightarrow \RR$ be the function of interest. 
Here $x = [x_1, \dots, x_D] \in \RR^D$ and $\Xcal \subset \RR^D$.
We have data $\XYn = \{(X_1, Y_1), \dots, (X_n, Y_n) \}$ and wish to obtain an estimate
$\funchat$ of $\func$.
In this work, we will be assuming that $\func$ has the form
\begin{align*}
\func(x) = \funcii{1}(\xii{1}) + \funcii{2}(\xii{2}) + \dots +
\funcii{M}(\xii{M}),
\numberthis
\label{eqn:addAssumption}
\end{align*}
where $\xii{j} \in \Xcalj \subset \RR^{d_j}$ and $\funcj:\Xcalj \rightarrow
\RR$. We shall refer to the $\Xcalj$'s as groups and the collection of all
groups $\bigcup_{j=1}^M \Xcalj$ as the decomposition.
In this work, we are particularly  interested in the case
where $D$ is very large and the group dimensionality is bounded-- i.e. $d_j \leq
d \ll D$. 

The work in \citet{hastie90gam} treats $\funchat$ as a sum of one
dimensional components. The decomposition here corresponds to
$\xii{j} = x_j$, $d_j = d =1\; \forall j$ and $M = D$. 
In this project, we would like to be more expressive than this model. We will
consider decompositions for which $d > 1$ and more importantly allows for
overlap between the groups.
\citet{ravikumar09spam} treat $\funchat$ as a sparse combination of one
dimensional functions. While this is seemingly restrictive than
\citep{hastie90gam}, the sparse approximation may provide favourable
bias-variance tradeoffs in high dimensions. Drawing inspiration from this, we
will consider models where $M$ is very large and seek a sparse collection of
groups to approximate the function - i.e. $\funchatj = \zero$ for several $j$.

In this work, we will, at least initially, be focusing on kernel regression. The Nadaraya
Watson estimator \cite{tsybakov08nonparametric} is a popular kernel smoothing method which
estimates the function via,
\[
\funchat(t) =  \frac{\sum_{i=1}^n\kernel(t,X_i) Y_i }{\sum_{i=1}^n \kernel(t,X_i)}
\]
A natural way to handle additive models is to use an additive kernel of the form
\[
\kernel(x,x') = 
\alpha_1\kernelii{1}(\xii{1}, {\xii{1}}') +
\alpha_2\kernelii{2}(\xii{2}, {\xii{2}}') + \dots
\alpha_M\kernelii{M}(\xii{M}, {\xii{M}}')
\]
The optimization problem here then is to learn 
$\alpha \in \RR^M$ and the hyperparameters of the kernels $\kernelj$ by minimizing the
cross validation error. If we fix the hyperparameters of the kernel, the problem
is convex in $\alpha$ but otherwise the problem is generally nonconvex.

As a first step, we wish to study the problem when $M$ is manageable. For this,
we could select the groups either randomly or via some greedy procedure.
Alternatively, \citep{duvenaud11additivegps} use a trick based on elementary
symmetric polynomials to efficiently compute additive kernels of up to all
orders of interaction. This however, will require some parameter sharing between
the kernels and not be as expressive. We shall first explore these paths before
delving deep into more sophisticated additive models.
Outside optimization, we also wish to study some of the statistical properties
of the function such as rate of convergence and minimaxity.

